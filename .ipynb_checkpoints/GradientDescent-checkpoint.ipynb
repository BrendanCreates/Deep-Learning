{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f780ff2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "!pip install otter-grader\n",
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add0ff4-27fa-4b8b-9429-77a6e70eb204",
   "metadata": {},
   "source": [
    "# FILL IN YOUR NAME AND THE NAME OF YOUR PEER (IF ANY) BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114b0f41-932d-43fb-96b0-a10ea759b6e4",
   "metadata": {},
   "source": [
    "**Name**: \\<replace this with your name\\>\n",
    "\n",
    "**Peer**: \\<replace this with your peer's name\\>\n",
    "\n",
    "## Collaboration policy\n",
    "Students are responsible for writing their own quizzes, assignments, and exams. For homework assignments, students are welcome (and encouraged) to discuss problems with one peer, **but each student must write their own assignment writeup and code individually**. The peer must be listed at the top of the writeup for each assignment. *Note: I will treat AI assistants as peers. That is, students are welcome to discuss problems with an AI assistant, but it is considered cheating to directly obtain an answer by querying the assistant. Please credit any AI assistant that you use.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ca860-333f-401d-839f-4a9081aa36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before starting\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeaa842-e2ff-4f0d-9447-6450943e8703",
   "metadata": {},
   "source": [
    "# Homework 2 -- Gradient Descent (100 pts)\n",
    "\n",
    "**Due:** Wednesday, September 10th, 2025 at 11:59 pm\n",
    "\n",
    "For this homework, it will be helpful to review the slides and your notes on Regression and on Gradient descent.\n",
    "\n",
    "We will use Jupyter/Colab notebooks throughout the semester for writing code and generating assignment outputs. \n",
    "\n",
    "Note that some questions have public tests, and others do not. These are only intended to provide insight into what may not be working in your solution, but they generally do not count toward your grade. Passing all public tests does not guarantee that you will obtain full marks for a problem. The per-question score given to you by Gradescope is the only feedback you will obtain about your grade. It is possible that `print` statements may cause your code to receive no marks for a given question, so I encourage you to remove/comment out any `print` statements before submitting your assignment. **If you pass public tests but fail to obtain full marks on Gradescope, it is your responsibility to debug your code further (e.g., by devising additional test cases).**\n",
    "\n",
    "## 1) 2D gradient descent (15 pts)\n",
    "\n",
    "Let's consider gradient descent when the dimension of the input is 2: $$\\theta=\\begin{bmatrix}\\theta_1\\\\\\theta_2\\end{bmatrix}\\enspace.$$ Suppose we have $f(\\theta)$ and we are trying to find the values of $\\theta_1$ and $\\theta_2$ that minimize it. Consider: $$f(\\theta)=-3\\theta_1 -\\theta_1\\theta_2 + 2\\theta_2 + \\theta_1^2 + \\theta_2^2\\enspace.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794950c-55b9-41e5-a0fb-6e4a2cd10ca2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.1)\n",
    "\n",
    "What is the first component of $\\nabla_\\theta f(\\theta)$? Enter a valid Python expression involving `theta_1` and `theta_2`, where both are scalar variables (i.e., single numbers).\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0888281-ada9-4e65-9d3a-70e3be6efe8a",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def grad_11(theta_1, theta_2):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ddb92",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdb5bc-542b-41bf-b47d-4a1deadf02e1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.2)\n",
    "\n",
    "What is the second component of $\\nabla_\\theta f(\\theta)$? Enter a valid python expression involving `theta_1` and `theta_2`, where both are scalar variables (i.e., single numbers).\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52debf8d-7313-46e4-bc3c-eef08ddd5085",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def grad_12(theta_1, theta_2):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21590d7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f85a4-d81e-4fbe-b9c0-00c291db019f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.3) \n",
    "What is $f\\left(\\begin{bmatrix}1\\\\1\\end{bmatrix}\\right)$? Enter a scalar value.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa9f0b-0af2-4ed4-af7a-3fa107189eae",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "f_13 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873bde76",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b468bed-d72e-4dd2-9ea9-086dba07fb4c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.4)\n",
    "If we started at $\\theta=\\begin{bmatrix}1\\\\1\\end{bmatrix}$ and took one step of gradient descent with step-size $\\eta=0.1$, what would the next value of $\\theta$ be? Enter a single (not nested) list representing $\\theta^\\top_{new}$ (as a row vector); i.e. `[theta_1_new, theta_2_new]`:\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635614f-37fe-404a-8bf7-d336daf5cb13",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "th_new_14 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c4a28",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae68e0-8664-4742-ad85-39734d382bf4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.5)\n",
    "\n",
    "What is $f(\\theta_{new})$? Enter a scalar value.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5adfa6-b06b-4d28-9a79-671a16b6bf50",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "f_15 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ade62a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b7e83-361d-49fa-9fa0-87df3437332c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.6)\n",
    "\n",
    "If we started at $\\theta=\\begin{bmatrix}1\\\\1\\end{bmatrix}$ and took one step of gradient descent with step-size $\\eta=1.0$ instead, what would the next value of $\\theta$ be? Enter a single (not nested) list representing $\\theta^\\top_{new}$ (as a row vector); i.e. `[theta_1_new, theta_2_new]`:\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e9393-3a37-4b6d-91b7-150404417afe",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "th_new_16 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3b9a6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dfa7a7-93a9-485c-8655-0ca1db8c4204",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.7)\n",
    "What is $f(\\theta_{new})$ now? Enter a numerical value.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81595515-2171-431d-b3cd-127be8cbdedc",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "f_17 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d71fc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4918ac6b-f888-4201-b955-b349e3c2c6eb",
   "metadata": {},
   "source": [
    "## 2) Alternative loss functions (15 pts)\n",
    "\n",
    "Although squared loss has a lot of good properties, it can also be\n",
    "very sensitive to \"outliers\" (points that may have been generated by\n",
    "some kind of underlying error in the data-generation process that we\n",
    "don’t really want to model).  We’ll consider some alternative loss\n",
    "functions and consider how they are affected by outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2673f3d4-669b-4a98-b2f4-44c39789d637",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.1)\n",
    "We could just penalize the _absolute difference_ between $g$ and $a$ (where $g$ denotes the guess and $a$ denotes the actual label of a data point), so $L_1(g, a)=|g - a|$. Does it penalize big diferences between $g$ and $a$:\n",
    "1. More than squared error?\n",
    "2. The same as squared error?\n",
    "3. Less than squared error?\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f6241-6634-4dd0-8eda-f236d972f9e2",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# set ans = 1 for (1), ans = 2 for (2), and ans = 3 for (3)\n",
    "ans_21 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155cf6c-05ad-491e-921f-899564075e1e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.2)\n",
    "We could consider an alternative called _Pseudo-Huber loss_, where $L_h(g, a) = \\sqrt{1 + (g - a)^2} - 1$. Does it penalize big diferences between $g$ and $a$:\n",
    "1. More than squared error?\n",
    "2. The same as squared error?\n",
    "3. Less than squared error?\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcffcdb4-a030-4d38-8fcb-3e07ff5d3aed",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# set ans = 1 for (1), ans = 2 for (2), and ans = 3 for (3)\n",
    "ans_22 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d1982-83c4-4fa7-b3da-8a2bd56508b4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.3)\n",
    "\n",
    "What is $\\frac{d}{dg} L_h (g, a)$? Enter a Python expression involving `g` and `a`. Use `**` for exponentiation and `np.sqrt(x)` for the square root of `x`.\n",
    "\n",
    "_Points:_ 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9528d81-90c0-49b6-8180-d21db784ce5a",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def derivative_huber_23(g, a):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c34fbe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137d9e2-5d32-4b21-8a10-0dfa9f888f75",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.4)\n",
    "\n",
    "Suppose that our hypothesis takes the form of a linear regressor without an offset parameter, $h(x; \\theta) = \\theta^\\top x$. If we wanted to use gradient descent to minimize a mean square loss objective without any regularization (which uses the squared loss function $L_2$ inside the sum), the gradient of the objective with respect to $\\theta$ takes the form $$\\nabla{_\\theta} \\left[\\frac{1}{n} \\sum_{i=1}^n \\underbrace{(\\theta^\\top x^{(i)} - y^{(i)})^2}_{:= L_2}\\right] = \\frac{1}{n} \\sum_{i=1}^n f(x^{(i)}, y^{(i)}; \\theta) \\enspace, $$\n",
    "where $f(x, y; \\theta) = 2 x(\\theta^\\top x - y)$.\n",
    "\n",
    "If we were using $L_h$ instead of $L_2$, the gradient would have the same form, but with a different function $f$.  Provide a definition\n",
    "for the $f$ associated with $L_h$. In other words, provide a definition for $\\nabla_{\\theta} L_h(\\theta^\\top x, y)$. Enter a Python expression involving `x`, `y` and `theta`. Use `**` for exponentiation, `np.sqrt(x)` for the square root of `x`, `x.T` for the transpose of `x` and `p@q` for the matrix product of `p` and `q`. Make sure that the output has the correct shape as a column vector ($d\\times 1$).\n",
    "\n",
    "_Hint: you already computed part of the chain rule in 2.3. It may help to explicitly write a variable `g` to contain the prediction under the linear hypothesis._\n",
    "\n",
    "_Points:_ 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae096c0-a759-4df8-adb7-f5e658626741",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def grad_huber_24(x, y, theta):\n",
    "    g = ...\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c44a0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c6e1a-bc3b-405d-9638-8f24c7da02ed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 3) Stochastic gradient descent for ridge regression (10 pts)\n",
    "\n",
    "Recall that the ridge-regression objective is $$J_{\\text{ridge}}(\\theta, \\theta_0) = \\left( \\frac{1}{n}\\sum_{i = 1}^n\\left(\\theta^\\top x^{(i)} + \\theta_0 - y^{(i)}\\right)^2 \\right) + \\lambda\\|\\theta\\|^2\\enspace.$$ \n",
    "\n",
    "We would like to do stochastic gradient descent to optimize this objective on a data set. To apply SGD, we need to describe our objective in the form $$\\sum_i f(x^{(i)}, y^{(i)};\\theta, \\theta_0)\\enspace.$$\n",
    "\n",
    "Provide a Python expression for $f(x, y; \\theta, \\theta_0)$ that makes this objective equivalent to $J_{\\text{ridge}}$. Your expression can include `theta`, `theta_0`, `x`, `y`, `lam` for $\\lambda$, and `n`. You may also need to use `.T` for matrix tranpose, `@` for matrix multiplication, `**` for exponentiation, and `*` for element-wise multiplication. Your expression should produce a $1\\times1$ numpy array.\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7cb2f6-368e-4b2c-ba92-c40024a47e0d",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def f_3(x, y, theta, theta_0, lam, n):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1daf48",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8def5-515a-45ef-a1e9-3eafb7af4562",
   "metadata": {},
   "source": [
    "## 4) Stochastic gradient descent in 1D (20 pts)\n",
    "\n",
    "Consider a 1D regression problem without an offset $\\theta_0$, which forces the graph of the hypothesis to go through the point $x=0, y=0$. There is one parameter $\\theta$ in this model, and our hypothesis is $h(x;\\theta)=\\theta x$, where we can treat $x$ and $\\theta$ as scalars for simplicity.\n",
    "\n",
    "Our data set $\\mathcal{D}=\\{(x^{(i)}, y^{(i)})\\}_{i=1}^n$ has $n=2$ data points: $$\\mathcal{D} = \\{(1, 1), (1, -1)\\}\\enspace.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6799e8d-e16d-4cd0-a3be-1e9717f4d825",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.1)\n",
    "\n",
    "What value of $\\theta$ minimizes the mean squared loss for this dataset? (You sould be able to solve for this $\\theta$ value by hand.)\n",
    "\n",
    "Enter a scalar value.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494b209-00ae-424f-946e-e4f11ef79c9b",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "theta_41 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91b0a0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f26f9a-2eff-4972-b822-898fc9d4a7a8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.2)\n",
    "\n",
    "Starting with an initial guess of $\\theta=1$, and letting the step size be $\\eta=0.2$, what are the values of $\\theta$ after the first step of batch gradient descent and after the second step of batch gradient descent? \"Batch\" gradient descent just means that we compute the gradient over the whole batch of data (unlike stochastic gradient descent, which computes the gradient over one data point). \n",
    "\n",
    "Enter your answer as a list of $\\theta$ values of the form `[theta1, theta2]`.\n",
    "\n",
    "_Points:_ 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb83cc-50c3-434c-8129-fae783edd249",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "thetas_42 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da26f7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd090b-db2e-4056-abf0-15bf21c4ec9e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.3)\n",
    "\n",
    "Now let's consider a variation of SGD.  Each step of SGD will pick exactly one datapoint, and will strictly alternate between our two data points (rather than randomly pick one datapoint to use).\n",
    "\n",
    "We will start with $\\theta=-1/9$ and use a step-size $\\eta=0.2$. Assume that we start with the data point $(1,1)$. Now, what are the values of $\\theta$ resulting from the first four steps of this modified SGD?\n",
    "\n",
    "Assume that our SGD takes a step of the form: $$\\theta' = \\theta - \\eta\\frac{2}{n}(\\theta x^{(i)} - y^{(i)})x^{(i)}\\enspace,$$ where $\\theta'$ is the new value for $\\theta$ in each step, and $i$ is selected alternately as descibed above.\n",
    "\n",
    "Enter your answer as a list of $\\theta$ values, entering fractions if possible.\n",
    "\n",
    "_Points:_ 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97977b65-0de1-4657-9525-2f8ca3614cf1",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "thetas_43 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9b768",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567217e-dc7d-457e-a1e6-48621ca7ec59",
   "metadata": {},
   "source": [
    "## 5) Implementing gradient descent (20 pts)\n",
    "\n",
    "In this section, we will implement generic versions of gradient descent and apply these to the linear regression (with regularization) objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac753612-886f-4cfa-939a-0149b8c0b2f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 5.1) Gradient descent\n",
    "\n",
    "We want to find the $x$ that minimizes the value of the _objective function_ $f(x)$, for an arbitrary scalar function $f$. The function $f$ will will be implemented as a Python function of one argument, that will be a numpy column vector. For efficiency, we will work with Python functions that return not just the value of $f$ at $x$, $f(x)$, but also return the gradient vector at $x$, that is, $\\nabla_x f(x)$. \n",
    "\n",
    "We will now implement a generic gradient descent function, `gd_51` that has the following input arguments:\n",
    "- `f`: a function whose input is a column vector `x` and returns a $1\\times1$ array\n",
    "- `df`: a function whose input is a column vector `x` and returns a column vector representing the gradient of `f` at `x`\n",
    "- `x0`: an initial value of $x$, which is a column vector\n",
    "- `step_size_fn`: a function that is given the iteration index (an integer starting at zero) and returns a step size.\n",
    "- `num_steps`: the number of update steps to perform\n",
    "\n",
    "Our function `gd_51` returns a tuple:\n",
    "- `x`: the value at the final step\n",
    "- `fx`: the value of `f(x)` at the final step\n",
    "\n",
    "**Hint:** This is a short function!\n",
    "\n",
    "Some test or example functions (which are made available as public tests with the autograder) that you may find useful are reproduced below. You may also find `rv` and `cv` (from previous weeks) useful, though not necessary.\n",
    "\n",
    "```\n",
    "def f1(x): # f(x,y,z) = y^2 + z\n",
    "    return x[1:2, :]**2 + x[2:3, :]\n",
    "\n",
    "def df1(x):\n",
    "    x = list(x.squeeze())\n",
    "    return cv([0, 2*x[1], 1])\n",
    "\n",
    "def f2(x): # f(x,y,z) = xy\n",
    "    return x[0:1, :]*x[1:2, :]\n",
    "\n",
    "def df2(x):\n",
    "    x = list(x.squeeze())\n",
    "    return cv([x[1], x[0], 0])\n",
    "\n",
    "def rv(values):\n",
    "    return np.array([values])\n",
    "\n",
    "def cv(values):\n",
    "    return rv(values).T\n",
    "```\n",
    "\n",
    "The test cases are provided below, but you should feel free (and are encouraged!) to write more of your own.\n",
    "\n",
    "```\n",
    "# Test case 1\n",
    "x1, fx1 = gd_51(f1, df1, cv([1.,1.,1.]), lambda i: 0.1, 1000)\n",
    "\n",
    "# Test case 2\n",
    "x2, fx2 = gd_51(f2, df2, cv([2., 3., 4.]), lambda i: 0.01, 1000)\n",
    "```\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99288195-c3b9-46b6-b305-95bac90182e4",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def gd_51(f, df, x0, step_size_fn, num_steps):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f68d6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4d7d5-cd4a-4f1f-a276-bd8f87878036",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 5.2) Numerical gradient\n",
    "\n",
    "Getting the analytic gradient correct for complicated functions is tricky. A very handy method for verifying the analytic gradient or even substituting for it is to estimate the gradient at a point by means of _finite differences_.\n",
    "\n",
    "Assume that we are given a function $f(x)$ that takes a column vector as its argument and returns a scalar value. In gradient descent, we will want to estimate the gradient of $f$ at a particular $x_0$. \n",
    "\n",
    "The $i$-th component of $\\nabla_x f(x_0)$ can be estimated as $$\\frac{f(x_0+\\delta_i) - f(x_0 - \\delta_i)}{2\\delta}\\enspace,$$ where $\\delta_i$ is a column vector whose $i$-th coordinate is $\\delta$ (a small constant such as $0.001$) and whose other components are zero. Note that adding or subtracting $\\delta_i$ is the same as incrementing or decrementing the $i$-th component of $x_0$ by $\\delta$, leaving the other components of $x_0$ unchanged. Using these results, we can estimate the $i$-th component of the gradient.\n",
    "\n",
    "For example, if $x_0 = [1, 1, \\ldots, 1]^\\top$ and $\\delta=0.01$, we may approximate the first component of $\\nabla_x f(x_0)$ as $$\\frac{f\\left(\\begin{bmatrix}1\\\\ 1\\\\ \\vdots \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} 0.01 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\right)- f\\left(\\begin{bmatrix}1\\\\ 1\\\\ \\vdots \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 0.01 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\right)}{0.02}\\enspace.$$\n",
    "\n",
    "**This process should be done for each dimension independently, and together the results of each computation are compiled to give the estimated gradient, which is $d$-dimensional.**\n",
    "\n",
    "Implement this as a function `make_num_grad_fn_52` that takes as arguments the objective function `f` and a value `delta`, and returns a new **function** that takes an `x` (a column vector of parameters) and returns a gradient column vector.\n",
    "\n",
    "**Hint 1:** All of your code will be within the function `df`. Can you see why? See [this](https://www.geeksforgeeks.org/python-closures/) for more information on returning a function instead of a value.\n",
    "\n",
    "**Hint 2:** If you do `temp_x = x` where `x` is a vector (numpy array), then `temp_x` is just another name for the same vector as `x` and changing an entry in one will change an entry in the other. You should either use `x.copy()` or remember to change entries back after modification.\n",
    "\n",
    "**Optional Challenge:** Assuming that the input function is implemented in a properly vectorized manner, can you implement the numerical gradient without using loops?\n",
    "\n",
    "We will use the same functions as in the previous exercise for testing. **Tip:** work out a simple 1D example and test your code (with print statements) to help with debugging!\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb4d47c-5ce0-4a6c-933e-db3fc4b0a203",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def make_num_grad_fn_52(f, delta=0.001):\n",
    "    def df(x):\n",
    "        ...\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe3b66",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa6ce0-1735-4df9-b9ac-f6aae382371e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 5.3) Using the numerical gradient\n",
    "\n",
    "Recall that our generic gradient descent function takes both a function `f` that returns the value of our function at a given point, and `df`, a function that returns a gradient at a given point. Write a function `minimize_53` that takes only a function `f` and uses this function and performs numerical gradient descent to return the local minimum (both `x` and `f(x)`).\n",
    "\n",
    "You can make use of your functions `gd_51` and `make_num_grad_fn_52` to implement `minimize`. You may use the default of `delta=0.001` for `make_num_grad_fn_52`.\n",
    "\n",
    "**Hint:** Your definition of `minimize_53` should call `make_num_grad_fn_52` exactly once to return a function. Then you may call the returned function many times in your updates for numerical gradient descent. You should return the same outputs as `gd_51`. \n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b224124b-0956-40a7-9383-0fe568943f74",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def minimize_53(f, x0, step_size_fn, num_steps, delta=0.001):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c56bd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be393434-67ed-4e0e-89a5-6690c9782c21",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 6) Stochastic gradient descent (20 pts)\n",
    "\n",
    "We will now write some general Python code to implement **stochastic gradient descent**.\n",
    "\n",
    "`sgd` takes the following as input: (Recall that the _stochastic_ part refers to using a randomly selected point and corresponding label from the given dataset to perform an update. Therefore, your objective function for a given step will need to take this into account.)\n",
    "- `X`: a standard data array (n by d)\n",
    "- `y`: a standard labels column vector (n by 1)\n",
    "- `J`: a cost function whose input is a data point (a column vector), a label (1 by 1) and a parameter vector `w` (a column vector) (in that order), and which returns a scalar. **Note:** the cost function `J` is not actually used when computing the (stochastic) gradient at each iteration. However, it may be useful to include as an argument for debugging purposes, (e.g., to print the cost function value at each iteration to observe its trend during the algorithm's execution).\n",
    "- `dJ`: a cost function gradient (corresponding to `J`) whose input is a data point (a column vector), a label (1 by 1) and a parameter vector `w` (a column vector) (also in that order), and which returns a column vector.\n",
    "- `w0`: an initial value of parameter vector $w$, which is a column vector.\n",
    "- `step_size_fn`: a function that is given the (zero-indexed) iteration index (an integer) and returns a step size.\n",
    "- `num_iter`: the number of iterations to perform\n",
    "\n",
    "\n",
    "It returns the following:\n",
    "- `w`: the value of the parameter vector at the final step (same dimensions as the input `w0`).\n",
    "\n",
    "**Note:** the parameter vector `w` being referenced above is the same as $\\theta$ we have used elsewhere\n",
    "(in the notes and in the other problems in this homework).\n",
    "\n",
    "You might find the function `np.random.randint(n)` useful in your implementation for sampling a random data point at each iteration.\n",
    "Note that you will be sampling _with replacement_. Sampling _without replacement_ is not necessary in this assignment (and makes things a little bit more complicated).\n",
    "\n",
    "**Hint:** This is a short function; the instructors' implementation is around 10 lines.\n",
    "\n",
    "**Hint:** Note that because we are using $X$ as a $n\\times d$ matrix, you might need to tranpose the vector you obtain when sampling an individual data point.\n",
    "\n",
    "_Points:_ 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f4256-060a-404e-9d44-a57d109ff566",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sgd_6(X, y, J, dJ, w0, step_size_fn, num_iter):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccaeb4b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f11fa64",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Fill out the answers to all questions and submit your file hw2.ipynb to the HW2 assignment on Gradescope. You are free to resubmit as many times as you wish. If the code below throws an error about not being able to generate a PDF, don't worry about it. Your notebook should still be okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d79bc8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42bb1ee",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "otter": {
   "assignment_name": "hw2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
